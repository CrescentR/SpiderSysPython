import asyncio
import json
import time
import re
import os
from typing import Dict, List
from urllib.parse import quote
from datetime import datetime
from .configs import AMQP_URL,DEFAULT_HEADERS, EXCHANGE_CONFIG, QUEUE_CONFIG
import aiohttp
import aio_pika
from bs4 import BeautifulSoup
from .broadcaster import Broadcaster
import random
os.environ["PYDEVD_USE_FRAME_EVAL"] = "NO"
def build_search_url(keywords: List[str], page_no: int, engine: str = "bing") -> str:
    """æ„å»ºæœç´¢å¼•æ“ URL"""
    if not keywords:
        return ""
    encoded_keywords = [quote(kw, safe='', encoding='utf-8') for kw in keywords]
    query = '+'.join(encoded_keywords)

    if engine == "baidu":
        pn = (page_no - 1) * 10 if page_no > 1 else 0
        return f"https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd={query}&pn={pn}"
    elif engine == "bing":
        first = (page_no - 1) * 10 + 1 if page_no > 1 else 1
        return f"https://www.cn.bing.com/search?q={query}&first={first}"
    return ""


def parse_links(html_content: str, engine: str = "bing") -> List[Dict[str, str]]:
    """ä» HTML ä¸­æå–é“¾æ¥ï¼ˆåŒæ­¥è§£æï¼Œä¸åšç½‘ç»œè¯·æ±‚ï¼‰"""
    soup = BeautifulSoup(html_content, 'html.parser')
    results = []

    if engine == "bing":
        items = (
                soup.find_all('li', class_='b_algo') or  # æ ‡å‡†ç»“æœ
                soup.find_all('div', class_='b_algo') or  # å¤‡é€‰
                soup.select('.b_algo')  # CSS é€‰æ‹©å™¨
        )
        for item in items:
            try:
                title_tag = item.find('h2')
                if not title_tag:
                    continue
                link_tag = title_tag.find('a')
                if not link_tag or not link_tag.get('href'):
                    continue
                source = item.find('cite')  # âš ï¸ cite æ ‡ç­¾é€šå¸¸åŒ…å«çœŸå®åŸŸå

                if title_tag and link_tag:
                    href = link_tag.get('href', '')

                    # ğŸ”¥ æ–¹æ³•1ï¼šä» cite æ ‡ç­¾è·å–çœŸå®é“¾æ¥
                    real_url = source.get_text(strip=True) if source else href

                    # ğŸ”¥ æ–¹æ³•2ï¼šå¦‚æœ href åŒ…å«çœŸå® URLï¼Œå°è¯•æå–
                    if 'bing.com/ck/' in href or not href.startswith('http'):
                        # å°è¯•ä»å…¶ä»–å±æ€§è·å–
                        real_url = link_tag.get('data-url') or link_tag.get('href')
                    else:
                        real_url = href

                    results.append({
                        'title': title_tag.get_text(strip=True),
                        'href': real_url,
                        'source': source.get_text(strip=True) if source else 'æœªçŸ¥æ¥æº',
                        'engine': 'bing'
                    })
            except Exception as e:
                print(f"è§£æå¤±è´¥: {e}")
                continue
    return results
class CrawlerService:
    """
    çˆ¬è™«æœåŠ¡ - æ”¯æŒå¤šç«¯æ¶ˆè´¹ï¼ˆFanout å¹¿æ’­ï¼‰
    """
    def __init__(self, amqp_url: str):
        self.amqp_url = amqp_url
        self.stop_flags: Dict[str, asyncio.Event] = {}
        self.connection = None
        self.channel = None
        self.exchange = None  # Fanout äº¤æ¢æœº
        self.cmd_queue = None
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0'
        ]

    def get_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        }
    async def initialize(self):
        """åˆå§‹åŒ– RabbitMQï¼šFanout Exchange + ä¸šåŠ¡é˜Ÿåˆ—ï¼›Topic Exchange + å‘½ä»¤é˜Ÿåˆ—"""
        self.connection = await aio_pika.connect(url=self.amqp_url)
        self.channel = await self.connection.channel()
        await self.channel.set_qos(prefetch_count=50)

        # å¹¿æ’­ Exchange
        self.exchange = await self.channel.declare_exchange(
            EXCHANGE_CONFIG["name"],
            EXCHANGE_CONFIG["type"],  # fanout
            durable=EXCHANGE_CONFIG["durable"]
        )

        # ç»‘å®šæ‰€æœ‰æ¶ˆè´¹é˜Ÿåˆ—
        for queue_key, config in QUEUE_CONFIG.items():
            queue = await self.channel.declare_queue(
                config["name"],
                durable=config["durable"],
                auto_delete=config["auto_delete"]
            )
            await queue.bind(self.exchange, routing_key="")
            print(f"âœ… é˜Ÿåˆ—å·²åˆ›å»ºå¹¶ç»‘å®š: {config['name']} ({queue_key})")

        # å‘½ä»¤é€šé“ï¼ˆTopicï¼‰
        cmd_exchange = await self.channel.declare_exchange(
            "crawler.command.exchange",
            aio_pika.ExchangeType.TOPIC,
            durable=True
        )
        self.cmd_queue = await self.channel.declare_queue(
            "crawler.command.queue",
            durable=True,
        )
        await self.cmd_queue.bind(cmd_exchange, routing_key="cmd.*")
        print("âœ… å‘½ä»¤é˜Ÿåˆ—å·²åˆ›å»º: crawler.command.queue")

    async def run(self):
        """è¿è¡Œä¸»å¾ªç¯ï¼šç›‘å¬å‘½ä»¤é˜Ÿåˆ—"""
        await self.initialize()

        print("ğŸš€ çˆ¬è™«æœåŠ¡å·²å¯åŠ¨ï¼ˆå¤šç«¯æ¶ˆè´¹æ¨¡å¼ï¼‰")
        print(f"ğŸ“¡ å¹¿æ’­äº¤æ¢æœº: {EXCHANGE_CONFIG['name']}")
        print(f"ğŸ“® æ¶ˆè´¹é˜Ÿåˆ—: {', '.join([c['name'] for c in QUEUE_CONFIG.values()])}\n")

        async with self.cmd_queue.iterator() as queue_iter:
            async for msg in queue_iter:
                async with msg.process():
                    try:
                        cmd = json.loads(msg.body)
                        task_id = cmd["task_id"]
                        if cmd["cmd"] == "start":
                            print(f"ğŸ“ æ”¶åˆ°å¯åŠ¨å‘½ä»¤: {task_id}")
                            self.stop_flags[task_id] = asyncio.Event()
                            asyncio.create_task(
                                self._start_job(self.exchange, cmd, self.stop_flags[task_id])
                            )
                        elif cmd["cmd"] == "stop":
                            print(f"ğŸ›‘ æ”¶åˆ°åœæ­¢å‘½ä»¤: {task_id}")
                            if task_id in self.stop_flags:
                                self.stop_flags[task_id].set()
                    except Exception as e:
                        print(f"âŒ å¤„ç†å‘½ä»¤å¤±è´¥: {e}")

    async def _start_job(self, exchange, cmd, stop_event: asyncio.Event):
        """å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
        task_id = cmd["task_id"]
        keywords = cmd["keywords"]
        total_pages = cmd["pageSize"]
        concurrency = cmd.get("concurrency", 1)
        rate = cmd.get("rateLimitPerSec", 2)
        engine = cmd.get("engine", "bing")

        print(f"ğŸ¯ å¼€å§‹ä»»åŠ¡ {task_id}: å…³é”®è¯={keywords}, é¡µæ•°={total_pages}")

        sem = asyncio.Semaphore(concurrency)
        last_ts = [time.time()]

        async def rate_limit():
            delta = time.time() - last_ts[0]
            if delta < 1.0 / rate:
                await asyncio.sleep(1.0 / rate - delta)
            last_ts[0] = time.time()

        try:
            connector = aiohttp.TCPConnector(
                ssl=False,  # æˆ–è€…ä½¿ç”¨ ssl=ssl.create_default_context()
                limit=10,  # é™åˆ¶å¹¶å‘è¿æ¥æ•°
                ttl_dns_cache=300
            )
            async with aiohttp.ClientSession(
                    connector=connector,
                    timeout=aiohttp.ClientTimeout(
                        total=30,  # å¢åŠ æ€»è¶…æ—¶
                        connect=10,  # è¿æ¥è¶…æ—¶
                        sock_read=20  # è¯»å–è¶…æ—¶
                    ),
                    headers=self.get_headers(),
                    cookie_jar=aiohttp.CookieJar()
            ) as session:
                # å¼€å§‹çŠ¶æ€ + åˆå§‹è¿›åº¦
                await Broadcaster.broadcast_status(exchange, task_id, "started")
                await Broadcaster.broadcast_progress(exchange, task_id, current=0, total=total_pages)

                tasks = []
                for page_no in range(1, total_pages + 1):
                    if stop_event.is_set():
                        break
                    url = build_search_url(keywords, page_no, engine=engine)

                    task = asyncio.create_task(
                        self._crawl_one(
                            session=session,
                            url=url,
                            page_no=page_no,
                            task_id=task_id,
                            exchange=exchange,
                            sem=sem,
                            rate_limit=rate_limit,
                            stop_event=stop_event,
                            engine=engine,
                            keywords=keywords,
                            total_pages=total_pages
                        )
                    )
                    tasks.append(task)

                await asyncio.gather(*tasks, return_exceptions=True)

                if not stop_event.is_set():
                    await Broadcaster.broadcast_status(exchange, task_id, "done")
                    print(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
                else:
                    await Broadcaster.broadcast_status(exchange, task_id, "stopped")
                    print(f"â¹ï¸ ä»»åŠ¡å·²åœæ­¢: {task_id}")

        except Exception as e:
            print(f"âŒ ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
            await Broadcaster.broadcast_status(exchange, task_id, "error", str(e))
        finally:
            if task_id in self.stop_flags:
                del self.stop_flags[task_id]

    async def _crawl_one(self, session, url, page_no, task_id,
                         exchange, sem, rate_limit, stop_event, engine, keywords, total_pages):
        """çˆ¬å–å•ä¸ªæœç´¢ç»“æœé¡µï¼Œå¹¶å¹¿æ’­æ¯æ¡é“¾æ¥"""
        if stop_event.is_set():
            return

        async with sem:
            await rate_limit()

            # ğŸ”¥ æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    async with session.get(url) as resp:
                        if resp.status != 200:
                            print(f"âš ï¸ é¡µé¢ {page_no} è¿”å›çŠ¶æ€ç : {resp.status}")
                            await Broadcaster.broadcast_progress(exchange, task_id, current=page_no, total=total_pages)
                            return
                        html = await resp.text(errors="ignore")

                        # ğŸ”¥ ä¿å­˜ HTML ç”¨äºè°ƒè¯•
                        debug_file = f"debug_bing_page{page_no}_task{task_id}.html"
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(html)
                        print(f"ğŸ” å·²ä¿å­˜è°ƒè¯•æ–‡ä»¶: {debug_file}")

                    links = parse_links(html, engine)
                    print(f"ğŸ“„ é¡µé¢ {page_no} æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                    print(f"ğŸ”— é“¾æ¥è¯¦æƒ…: {links}")  # ğŸ”¥ ç¡®ä¿æ‰“å°

                    for link in links:
                        if stop_event.is_set():
                            break
                        data = {
                            "task_id": task_id,
                            "keywords": keywords,
                            "url": link['href'],
                            "title": link['title'],
                            "source": link['source'],
                            "dateTime": datetime.now().isoformat(),
                        }
                        await Broadcaster.broadcast_result(exchange, task_id, data)

                    await Broadcaster.broadcast_progress(exchange, task_id, current=page_no, total=total_pages)
                    return  # ğŸ”¥ æˆåŠŸåç›´æ¥è¿”å›

                except (aiohttp.ClientError, asyncio.TimeoutError, ConnectionError) as e:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿: 1s, 2s, 4s
                        print(f"âš ï¸ é¡µé¢ {page_no} å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}, {wait_time}ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"âŒ é¡µé¢ {page_no} æœ€ç»ˆå¤±è´¥: {e}")
                        return
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_no} æœªçŸ¥é”™è¯¯: {e}")
                    return
async def main():
    svc = CrawlerService(AMQP_URL)
    await svc.run()

if __name__ == "__main__":  # æ³¨æ„ï¼šç”¨ -m è¿è¡Œæ—¶ï¼Œè¿™é‡Œä¼šè§¦å‘
    asyncio.run(main())