import asyncio
from playwright.async_api import async_playwright
from urllib.parse import quote_plus, urlencode
import random
import time

BING_BASE = "https://www.bing.com/search"

# å¯é€‰ï¼šæƒ³è¦å½»åº•ä¸å‡ºçŸ¥ä¹ï¼Œç›´æ¥åŠ ä¸Šè´Ÿè¯
EXCLUDE_SITES = ["zhihu.com", "baidu.com", "tieba.baidu.com"]

# # å¯é€‰ï¼šåªæƒ³è¦å£çº¸ç«™ï¼Œæ‰“å¼€è¿™ä¸ªç™½åå•
# WHITELIST_SITES = [
#     "wallhaven.cc", "unsplash.com", "pexels.com", "pixabay.com",
#     "alpha.wallhaven.cc", "deviantart.com", "artstation.com",
#     "simpledesktops.com", "wallpaperflare.com", "wallpaperhub.app",
# ]

def build_query(keywords, exclude_sites=None, whitelist_sites=None):
    """
    keywords: str æˆ– list[str]
    """
    if isinstance(keywords, (list, tuple)):
        q = " ".join(keywords)
    else:
        q = str(keywords)

    # è´Ÿè¯æ’é™¤åŸŸ
    if exclude_sites:
        q += " " + " ".join([f"-site:{d}" for d in exclude_sites])

    # åªè¦ç™½åå•ï¼ˆå¯é€‰ï¼‰
    if whitelist_sites:
        q += " " + " OR ".join([f"site:{d}" for d in whitelist_sites])

    return q.strip()


async def fetch_bing_results(keywords, page_no: int, *, mkt="en-US", cc="US",
                             use_english_results=True, count=10, timeout_ms=15000,
                             exclude_sites=None):  # æ–°å¢å‚æ•°
    """
    å¢åŠ  exclude_sites å‚æ•°ï¼Œåœ¨è§£æç»“æœæ—¶æ‰‹åŠ¨è¿‡æ»¤
    """
    q = build_query(keywords, exclude_sites=exclude_sites, whitelist_sites=None)
    params = {
        "q": q,
        "count": count,
        "first": page_no * count + 1,
        "form": "PERE",
        "mkt": mkt,
        "cc": cc,
    }
    if use_english_results:
        params["ensearch"] = "1"

    url = f"{BING_BASE}?{urlencode(params)}"

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            locale="en-US" if use_english_results else "zh-CN",
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            extra_http_headers={
                "Accept-Language": "en-US,en;q=0.9" if use_english_results else "zh-CN,zh;q=0.9",
            },
        )

        page = await context.new_page()
        page.set_default_timeout(timeout_ms)

        await page.goto(url)
        await page.wait_for_selector("li.b_algo h2 a", timeout=timeout_ms)

        nodes = await page.query_selector_all("li.b_algo h2 a")

        results = []
        seen = set()

        for a in nodes:
            try:
                title = (await a.inner_text()).strip()
                href = await a.get_attribute("href")
                if not href:
                    continue

                # ğŸ”¥ æ‰‹åŠ¨è¿‡æ»¤æ’é™¤ç«™ç‚¹
                if exclude_sites:
                    skip = False
                    for domain in exclude_sites:
                        if domain in href:
                            skip = True
                            break
                    if skip:
                        continue

                key = (title, href)
                if key in seen:
                    continue
                seen.add(key)

                results.append({"title": title, "href": href})
            except Exception:
                continue

        await context.close()
        await browser.close()
        return results


async def crawl_pages(keywords, total_pages: int, exclude_sites=None, **kwargs):
    """ä¼ é€’ exclude_sites åˆ°æ¯ä¸ªè¯·æ±‚"""
    tasks = []
    for page_no in range(total_pages):
        await asyncio.sleep(random.uniform(0.2, 0.6))
        tasks.append(fetch_bing_results(
            keywords, page_no,
            exclude_sites=exclude_sites,  # ä¼ é€’å‚æ•°
            **kwargs
        ))

    pages = await asyncio.gather(*tasks, return_exceptions=True)

    merged, seen = [], set()
    for page in pages:
        if isinstance(page, Exception):
            continue
        for item in page:
            key = item["href"]
            if key not in seen:
                seen.add(key)
                merged.append(item)
    return merged


async def start_crawl_task(keywords, total_pages: int, exclude_sites=None, **kwargs):
    """ä¼ é€’ exclude_sites"""
    results = await crawl_pages(keywords, total_pages, exclude_sites=exclude_sites, **kwargs)
    print(f"\nâœ… å…±è·å– {len(results)} ä¸ªæœ‰æ•ˆç»“æœ\n")
    for i, link in enumerate(results, 1):
        print(f"{i}. Title: {link['title']}")
        print(f"   Link: {link['href']}\n")


async def main():
    # æ–¹æ¡ˆ Aï¼šè‹±æ–‡å…³é”®è¯ + æ‰‹åŠ¨è¿‡æ»¤çŸ¥ä¹
    keywords = ["4K wallpaper"]
    await start_crawl_task(
        keywords,
        total_pages=3,  # å…ˆæµ‹è¯• 3 é¡µ
        exclude_sites=EXCLUDE_SITES,  # ğŸ”¥ ä¼ é€’æ’é™¤åˆ—è¡¨
        mkt="en-US",
        cc="US",
        use_english_results=True,
        count=10
    )


if __name__ == "__main__":
    asyncio.run(main())